{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "HIDDEN_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for debugging\n",
    "# import os\n",
    "# data_dir = os.path.join(os.getenv('TF_EXP_BASE_DIR', ''), 'dbpedia_data')\n",
    "# data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "CPU times: user 6.88 s, sys: 141 ms, total: 7.02 s\n",
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If this doesn't run you can download it from here:\n",
    "#https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
    "\n",
    "# Prepare training and testing data\n",
    "dbpedia = learn.datasets.load_dataset('dbpedia',size='small')\n",
    "\n",
    "x_train = pandas.DataFrame(dbpedia.train.data)[1]\n",
    "y_train = pandas.Series(dbpedia.train.target)\n",
    "x_test = pandas.DataFrame(dbpedia.test.data)[1]\n",
    "y_test = pandas.Series(dbpedia.test.target)\n",
    "\n",
    "print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The British Rail Class 421 (or 4Cig) electrical multiple units were built at BR York Works between 1964 and 1972. Units were built in two batches and were initially introduced on services on the Brighton Main Line. Later units were introduced on services to Portsmouth. These units replaced older Southern Railway-designed units such as the 5Bel Brighton Belle units and 4Cor units.\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "idx = 25\n",
    "print(x_test[idx])\n",
    "print(y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\r\n",
      "EducationalInstitution\r\n",
      "Artist\r\n",
      "Athlete\r\n",
      "OfficeHolder\r\n",
      "MeanOfTransportation\r\n",
      "Building\r\n",
      "NaturalPlace\r\n",
      "Village\r\n",
      "Animal\r\n"
     ]
    }
   ],
   "source": [
    "!head dbpedia_data/dbpedia_csv/classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.9 ms, sys: 8.92 ms, total: 71.8 ms\n",
      "Wall time: 83.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process character vocabulary\n",
    "char_processor = learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train = np.array(list(char_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(char_processor.transform(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32  84  89 ..., 105 114 105]\n",
      " [ 32  80  97 ..., 110  32  69]\n",
      " [ 32  78  73 ..., 102  32  65]\n",
      " ..., \n",
      " [ 32  83 105 ..., 114 116  32]\n",
      " [ 32  68  97 ..., 101 100  32]\n",
      " [ 32  76  97 ..., 114 110  97]]\n",
      "(70, 100)\n"
     ]
    }
   ],
   "source": [
    "print(x_test)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 9 µs, total: 20 µs\n",
      "Wall time: 39.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def char_rnn_model(x, y):\n",
    "    \"\"\"Character level recurrent neural network model to predict classes.\"\"\"\n",
    "    \n",
    "    # set up output encoding\n",
    "    y = tf.one_hot(y, 15, 1, 0)\n",
    "    \n",
    "    # set up character matrix as one hot\n",
    "    byte_list = learn.ops.one_hot_matrix(x, 256)\n",
    "    byte_list = tf.unpack(byte_list, axis=1)\n",
    "    print(len(byte_list))\n",
    "\n",
    "    # create RNN cell type\n",
    "    cell = tf.nn.rnn_cell.GRUCell(HIDDEN_SIZE)\n",
    "    #cell = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "    \n",
    "    # define inputs and unroll cells\n",
    "    rnn_output_list, final_encoding = tf.nn.rnn(cell, byte_list, dtype=tf.float32)\n",
    "    print(final_encoding)\n",
    "\n",
    "    # use encoding to predict via fully connected layer\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, y)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/t4/hzf28kx94nv2vp_fvfsfvkdw0000gn/T/tmpwqb9q7r3\n",
      "WARNING:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Tensor(\"RNN/GRUCell_99/add:0\", shape=(?, 20), dtype=float32)\n",
      "100\n",
      "Tensor(\"RNN/GRUCell_99/add:0\", shape=(?, 20), dtype=float32)\n",
      "Accuracy: 0.285714\n",
      "CPU times: user 6min 7s, sys: 1min 2s, total: 7min 9s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build model\n",
    "classifier = learn.Estimator(model_fn=char_rnn_model)\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=200)\n",
    "y_predicted = [p['class'] for p in classifier.predict(x_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "Example adjusted from:\n",
    "- https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/skflow/text_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 7552\n"
     ]
    }
   ],
   "source": [
    "# What about using word embeddings rather than char embeddings\n",
    "MAX_DOCUMENT_LENGTH = 30\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "# Prepare training and testing data\n",
    "dbpedia = learn.datasets.load_dataset('dbpedia',size='small')\n",
    "\n",
    "x_train = pandas.DataFrame(dbpedia.train.data)[1]\n",
    "y_train = pandas.Series(dbpedia.train.target)\n",
    "x_test = pandas.DataFrame(dbpedia.test.data)[1]\n",
    "y_test = pandas.Series(dbpedia.test.target)\n",
    "\n",
    "# Process vocabulary\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(vocab_processor.transform(x_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_rnn_model(x, y):\n",
    "    \"\"\"Recurrent neural network model to predict from sequence of words\n",
    "    to a class.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "\n",
    "    # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "    #cell = tf.nn.rnn_cell.GRUCell(EMBEDDING_SIZE)\n",
    "    #cell = tf.nn.rnn_cell.BasicLSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(EMBEDDING_SIZE,state_is_tuple=False)\n",
    "    #cell = tf.nn.rnn_cell.BasicRNNCell(HIDDEN_SIZE)\n",
    "\n",
    "    # Create an unrolled Recurrent Neural Networks to length of\n",
    "    # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "    per_rnn_output, final_encoding = tf.nn.rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "    # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "    # neural network of last step) and pass it as features for logistic\n",
    "    # regression over output classes.\n",
    "    target = tf.one_hot(y, 15, 1, 0)\n",
    "    prediction, loss = learn.models.logistic_regression(final_encoding, target)\n",
    "\n",
    "    # Create a training op.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/t4/hzf28kx94nv2vp_fvfsfvkdw0000gn/T/tmpziuixhn3\n",
      "WARNING:tensorflow:Using default config.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x1190acba8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x10421def0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.600000\n",
      "CPU times: user 3min 46s, sys: 32.2 s, total: 4min 19s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build model\n",
    "classifier = learn.Estimator(model_fn=word_rnn_model)\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=200)\n",
    "y_predicted = [p['class'] for p in classifier.predict(x_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/t4/hzf28kx94nv2vp_fvfsfvkdw0000gn/T/tmpaar68r74\n",
      "WARNING:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.814286\n",
      "CPU times: user 52.7 s, sys: 7.11 s, total: 59.8 s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# shorter syntax for basic cell configurations\n",
    "# this example hides much of the configuratbility \n",
    "# and hides the process of cell creation, so...\n",
    "# I wouldn't start with this one\n",
    "\n",
    "def input_op_fn(x):\n",
    "    \"\"\"Customized function to transform batched x into embeddings.\"\"\"\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = learn.ops.categorical_variable(x, n_classes=n_words,\n",
    "      embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = tf.unpack(word_vectors, axis=1)\n",
    "    return word_list\n",
    "\n",
    "classifier = learn.TensorFlowRNNClassifier(\n",
    "        rnn_size=EMBEDDING_SIZE, n_classes=15, cell_type='gru',\n",
    "        input_op_fn=input_op_fn, num_layers=1, bidirectional=False,\n",
    "        sequence_length=None, optimizer='Adam',\n",
    "        learning_rate=0.01, continue_training=True)\n",
    "\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=200)\n",
    "y_predicted = classifier.predict(x_test)\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [MLEnv]",
   "language": "python",
   "name": "Python [MLEnv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
