{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Agenda\n",
    " - Numerical Optimization Techniques\n",
    "  - Types of Optimization\n",
    "  - Programming the Optimization\n",
    " - **Whirlwind Lecture Alert**\n",
    "  - Entire classes cover these concepts in expanded form\n",
    "  - But you are smart enough to get them in one lecture!\n",
    "    - Because science!\n",
    "    \n",
    "___\n",
    "\n",
    "# Last Time\n",
    "\n",
    "|Description| Equations, Derivations, Hessian Calculations, and Miscellaneous|\n",
    "|-----------|--------|\n",
    "| Sigmoid Definition | $$ p(y^{(i)}=1\\text{ | }\\mathbf{x}^{(i)},\\mathbf{w})=\\frac{1}{1+\\exp{(-\\mathbf{w}^T \\mathbf{x}^{(i)}})}$$ |\n",
    "| Log Likelihood | $$ l(\\mathbf{w}) = \\sum_i \\left( y^{(i)} \\ln [g(\\mathbf{w}^T \\mathbf{x}^{(i)})] + (1-y^{(i)}) (\\ln [1 - g(\\mathbf{w}^T \\mathbf{x}^{(i)})])  \\right)  $$ |\n",
    "| Vectorized Gradient | $$gradient =\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}$$ |\n",
    "| Regularization | $$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\left[\\underbrace{\\nabla l(\\mathbf{w})_{old}}_{\\text{old gradient}} - C \\cdot 2\\mathbf{w} \\right]$$|\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PDF_slides/mark_scooter.png\"  width=\"300\">\n",
    "\n",
    "\n",
    "# More Advanced  Optimization for Machine Learning\n",
    "From previous notebooks, we know that the logistic regression update equation is given by:\n",
    "\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{old value}} + \\eta \\underbrace{\\left[\\left(\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))x^{(i)}_j\\right) - C \\cdot 2w_j \\right]}_{\\nabla l(w)}$$\n",
    "\n",
    "Which can be made into more generic notation by denoting the objective function as $l(\\mathbf{w})$ and the gradient calculation as $\\nabla$:\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\nabla l(\\mathbf{w})$$\n",
    "\n",
    "<img src=\"PDF_slides/batch.gif\"  width=\"400\">\n",
    "\n",
    "One problem is that we still need to set the value of $\\eta$, which can drastically change the performance of the optimization algorithm. If $\\eta$ is too large, the algorithm might be unstable. If $\\eta$ is too small, it might take a long time (i.e., many iterations) to converge.\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\underbrace{\\eta}_{\\text{best step?}} \\nabla l(\\mathbf{w}) $$\n",
    "\n",
    "We can solve this issue by performing a line search for the best value of $\\eta$ along the direction of the gradient, as denoted by:\n",
    "\n",
    "$$ \\eta \\leftarrow \\arg\\max_\\eta \\underbrace{\\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))^2 -C\\cdot\\sum_j w_j^2}_{\\nabla l(\\mathbf{w})} $$\n",
    "\n",
    "<img src=\"PDF_slides/line_search.gif\"  width=\"400\">\n",
    "\n",
    "# Optimizing Logistic Regression via Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(np.int) # make problem binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing \n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.12616889]\n",
      " [-0.29791217]\n",
      " [-0.37876886]\n",
      " [ 0.51370512]\n",
      " [ 0.32218262]]\n",
      "Accuracy of:  0.9733333333333334\n",
      "CPU times: user 17.2 ms, sys: 3.45 ms, total: 20.7 ms\n",
      "Wall time: 19.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,C):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization inopposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/50} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize\n",
    "                \n",
    "            \n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=0.01,iterations=40, C=0.001)\n",
    "\n",
    "lslr.fit(X,y)\n",
    "\n",
    "yhat = lslr.predict(X)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs well, but was not too much faster than previously (this is because $\\eta$ was chosen well in the initial example). \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Test\n",
    "<img src=\"PDF_slides/self_test_multiplies.png\"  width=\"400\">\n",
    "_____\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "Sometimes the gradient calcualtion is too computational:\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta\\left( \\sum_{i=1}^M (y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)} -2C\\cdot \\mathbf{w}\\right) $$\n",
    "\n",
    "Instead, we can approximate the gradient using one instance, this is called stochastic gradient descent (SGD) because the steps can appear somewhat random.\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\underbrace{\\left((y^{(i)}-g(\\mathbf{w}^T\\mathbf{x}^{(i)}))\\mathbf{x}^{(i)}-2C\\cdot \\mathbf{w}\\right)}_{\\text{approx. gradient}} \\text{,   where   } i\\in M$$\n",
    "\n",
    "<img src=\"PDF_slides/SGD.gif\"  width=\"400\">\n",
    "\n",
    "Let's code up the SGD example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.55807207]\n",
      " [-1.14734635]\n",
      " [-1.30229895]\n",
      " [ 2.00012099]\n",
      " [ 1.38896833]]\n",
      "Accuracy of:  0.9333333333333333\n",
      "CPU times: user 18.9 ms, sys: 8.78 ms, total: 27.7 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(0.05,500, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<img src=\"PDF_slides/BtJXjJcCAAE7QOB.jpg\"  width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "# Optimizing with Second Order Derivatives\n",
    "First, let's look at the one dimensional case when we have a function $l(w)$ where w is a scalar. The optimal value of w is given by:\n",
    "\n",
    "$$ w \\leftarrow w - \\underbrace{[\\frac{\\partial^2}{\\partial w}l(w)]^{-1}}_{\\text{inverse 2nd deriv}}\\underbrace{\\frac{\\partial}{\\partial w}l(w)}_{\\text{derivative}}  $$\n",
    "\n",
    "Note that if $l(w)$ is a quadrativ function, this solution converges in a single step!\n",
    "\n",
    "\n",
    "|Aside: an example with the second derivative:|\n",
    "|------------------------------------------------------------------------|\n",
    "|Say $l(w)=2w^2+4w+5$, and we want to minimize the function. We have that:\n",
    "|$$\\frac{\\partial}{\\partial w}l(w)=4w+4$$|\n",
    "| $$\\frac{\\partial^2}{\\partial w}l(w)=4$$|\n",
    "|Therefore, if we choose $w_{start}=0$, we have:|\n",
    "|$$\\frac{\\partial}{\\partial w}l(0)=4$$  |\n",
    "|$$\\frac{\\partial^2}{\\partial w}l(0)=4$$ |\n",
    "|So the update becomes|\n",
    "|$$w \\leftarrow w_{start} - \\frac{1}{4}4 = -1$$|\n",
    "|The solution is found in one step. This works for any initial value of $w_{start}$. Let's verify that the solution worked graphically.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(-1, 2.5, '$\\\\leftarrow$found minimum')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "w = np.linspace(-7,5,100)\n",
    "l = 2*w**2+4*w+5\n",
    "plt.plot(w,l)\n",
    "plt.text(-1,2.5,'$\\leftarrow$found minimum',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Newton's Update Method\n",
    "<img src=\"PDF_slides/newton.png\"  width=\"600\">\n",
    "\n",
    "But how do we translate this over to objective funtions with more than one variable? We need a second derivative of a multivariate equation... enter, the hessian. Our new update is defined by Newton's method:\n",
    "\n",
    "$$ w \\leftarrow w - \\underbrace{[\\frac{\\partial^2}{\\partial w}l(w)]^{-1}}_{\\text{inverse 2nd deriv}}\\underbrace{\\frac{\\partial}{\\partial w}l(w)}_{\\text{derivative}}  $$\n",
    "\n",
    "to\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\mathbf{H}[l(\\mathbf{w})]^{-1}}_{\\text{inverse Hessian}}\\cdot\\underbrace{\\nabla l(\\mathbf{w})}_{\\text{gradient}}$$\n",
    "\n",
    "where the Hessian is defined as follows for any multivariate equation $l(\\mathbf{w})$:\n",
    "$$ \\nabla^2 l(\\mathbf{w}) = \\mathbf{H}[l(\\mathbf{w})]   $$\n",
    "\n",
    "$$  \\mathbf{H}[l(\\mathbf{w})] =  \\begin{bmatrix}\n",
    "        \\frac{\\partial^2}{\\partial w_1}l(\\mathbf{w}) &  \\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_2}l(\\mathbf{w}) & \\ldots     & \\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_N}l(\\mathbf{w})  \\\\\n",
    "        \\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_1}l(\\mathbf{w})  & \\frac{\\partial^2}{\\partial w_2}l(\\mathbf{w}) &  \\ldots     & \\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_N}l(\\mathbf{w})  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\frac{\\partial}{\\partial w_N}\\frac{\\partial}{\\partial w_1}l(\\mathbf{w})  & \\frac{\\partial}{\\partial w_N}\\frac{\\partial}{\\partial w_2}l(\\mathbf{w}) &  \\ldots     & \\frac{\\partial^2}{\\partial w_N}l(\\mathbf{w}) \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "For logistic regression:\n",
    "\n",
    "<img src=\"PDF_slides/hessian_derive.png\"  width=\"400\">\n",
    "\n",
    "Therefore the Hessian for logistic regression becomes:\n",
    "$$ \\mathbf{H}_{j,k}[l(\\mathbf{w})] = -\\sum_{i=1}^M g(\\mathbf{w}^T\\mathbf{x}^{(i)})[1-g(\\mathbf{w}^T\\mathbf{x}^{(i)})]{x_k}^{(i)}{x_j}^{(i)} + \\underbrace{2\\cdot C}_{\\text{regularization}}  $$\n",
    "\n",
    "You can see the full derivation of the Hessian in my notes here:\n",
    "- https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/HessianCalculation.pdf\n",
    "\n",
    "$$ \\mathbf{H}[l(\\mathbf{w})] =  \\mathbf{X}^T \\cdot \\text{diag}\\left[g(\\mathbf{X}\\cdot\\mathbf{w})*(1-g(\\mathbf{X}\\cdot\\mathbf{w}))\\right]\\cdot \\mathbf{X} -2C$$\n",
    "\n",
    "Now we can place the Hessian derivation into the Newton Update Equation, like this:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\mathbf{H}[l(\\mathbf{w})]^{-1}}_{\\text{inverse Hessian}}\\cdot\\underbrace{\\nabla l(\\mathbf{w})}_{\\text{gradient}}$$\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\underbrace{\\left[\\mathbf{X}^T \\cdot \\text{diag}\\left[g(\\mathbf{X}\\cdot\\mathbf{w})(1-g(\\mathbf{X}\\cdot\\mathbf{w}))\\right] \\cdot \\mathbf{X} -2C \\right]^{-1} }_{\\text{inverse Hessian}} \\cdot \\underbrace{\\mathbf{X}*y_{diff}}_{\\text{gradient}}$$\n",
    "\n",
    "So let's code this up using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-4.75308496]\n",
      " [-0.17462172]\n",
      " [ 0.79365037]\n",
      " [ 0.0172447 ]\n",
      " [ 2.18414582]]\n",
      "Accuracy of:  0.9266666666666666\n",
      "CPU times: user 2.06 ms, sys: 1.58 ms, total: 3.64 ms\n",
      "Wall time: 2.77 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "       \n",
    "hlr = HessianBinaryLogisticRegression(eta=1.0,iterations=1,C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X,y)\n",
    "yhat = hlr.predict(X)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Problems With the Hessian\n",
    "Quadratic isn’t always a great assumption:\n",
    " - highly dependent on starting point\n",
    "  - jumps can get really random!\n",
    " - near saddle points, inverse hessian unstable\n",
    " - hessian not always invertible… or invertible with correct numerical precision\n",
    " \n",
    "The Hessian can sometimes be ill formed for these problems and can also be highly computational. Thus, we need to approximate the Hessian, and also use some heuristics (like momentum) to better control the steps we make and directions we use.  \n",
    "\n",
    "## Quasi-Newton Methods\n",
    "In general:\n",
    " - approximate the Hessian with something numerically sound and efficiently invertible \n",
    " - back off to gradient descent when the approximate hessian is not stable\n",
    " - use momentum to update approximate hessian\n",
    " - A popular approach: use Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
    "\n",
    "### BFGS\n",
    "One of the most popular quasi-Newton methods is known as Broyden-Fletcher-Goldfarb-Shanno (BFGS). We won't explicitly program the BFGS algorithm--instead we can take advantage of scipy's calculations to do it for us. For using this algorithm, we need to define the objective function and the gradient explicitly for another program to calculate. \n",
    "\n",
    "- https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm \n",
    "\n",
    "Essentially, we want to update the Hessian with an approximation that is easily invertible and based on stable gradient calculations. We can define the approximate Hessian for each iteration, $\\mathbf{H}_k$. \n",
    "\n",
    "|Description| Equations, Derivations, Hessian Calculations, and Miscellaneous |\n",
    "|-----------|--------|\n",
    "|1. Initial Approx. Hessian for $k=0$ is identity matrix| $$\\mathbf{H}_0=\\mathbf{I}$$|\n",
    "|2. Find update direction, $p_k$ | $$ p_k = -\\mathbf{H}_k^{-1} \\nabla l(w_k) $$| \n",
    "|3. Update $w_k$|$$w_{k+1}\\leftarrow w_k + \\eta \\cdot p_k $$|\n",
    "|4. Save scaled direction| $$s_k=\\eta \\cdot p_k$$ |\n",
    "|5. Approximate change in derivative | $$v_k = \\nabla l(w_{k+1}) - \\nabla l(w_k) $$|\n",
    "|6. Redefine approx Hessian| $$\\mathbf{H}_{k+1}=\\mathbf{H}_k+\\underbrace{\\frac{v_k v_k^T}{v_k^T s_k}}_{\\text{approx. Hessian}} -\\underbrace{\\frac{\\mathbf{H}_k s_k s_k^T \\mathbf{H}_k}{s_k^T \\mathbf{H}_k s_k}}_{\\text{momentum}} $$ |\n",
    "|7. Approximate Inverse $\\mathbf{H}_{k+1}$| $$ \\mathbf{H}_{k+1}^{-1} = \\mathbf{H}_{k}^{-1} + \\frac{(s_k^T v_k+\\mathbf{H}_{k}^{-1})(s_ks_k^T)}{(s_k^T v_k)^2}-\\frac{\\mathbf{H}_{k}^{-1}v_ks_k^T+s_kv_k^T\\mathbf{H}_{k}^{-1}}{s_k^T v_k} $$|\n",
    "| 8. Repeat starting at step 2| $$ k = k+1 $$| \n",
    "\n",
    "Of course, we do not need to program this method because it already exists. We just need to understand the API to get the full functionality. Recall that Logistic regression uses the following objective function:\n",
    "\n",
    "$$ l(w) = \\left(\\sum_i y^{(i)} \\ln g(x^{(i)}) + (1-y^{(i)})\\ln[1-g(x^{(i)})]\\right)  - C \\cdot \\sum_j w_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.11114639]\n",
      " [-0.26762263]\n",
      " [-0.37781461]\n",
      " [ 0.49330217]\n",
      " [ 0.28629542]]\n",
      "Accuracy of:  0.98\n",
      "CPU times: user 4.14 ms, sys: 1.84 ms, total: 5.98 ms\n",
      "Wall time: 4.62 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSBinaryLogisticRegression(_,2,C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X,y)\n",
    "yhat = bfgslr.predict(X)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(bfgslr.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS for Multiclass Logistic Regression\n",
    "Now let's add BFGS to non-binary classification. As before, we will use one-versus-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "\n",
    "X = StandardScaler().fit(X).transform(X)\n",
    "y_not_binary = ds.target # note problem is NOT binary anymore, there are three classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-2.10666758 -1.37710387  2.12177587 -2.33013903 -2.18114396]\n",
      " [-0.86687856  0.2681656  -1.48158254  0.37887989 -0.70883214]\n",
      " [-5.54625821  1.14495766  0.17809041  3.19612685  4.55144602]]\n",
      "Accuracy of:  0.9333333333333333\n",
      "CPU times: user 11.8 ms, sys: 2.58 ms, total: 14.4 ms\n",
      "Wall time: 12.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(_,iterations=10,C=0.001)\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0587411   1.22287507 -1.76441737 -1.63742564]\n",
      " [ 0.13008673 -1.26245048  0.79595969 -0.89205903]\n",
      " [ 0.13994171 -0.51269087  2.48202553  3.14075251]]\n",
      "Accuracy of:  0.9466666666666667\n",
      "CPU times: user 22.6 ms, sys: 2.49 ms, total: 25.1 ms\n",
      "Wall time: 19.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='lbfgs',n_jobs=1) # all params default\n",
    "# note that sklearn is optimized for using the liblinear library with logistic regression\n",
    "# ...and its faster than our implementation here\n",
    "\n",
    "lr_sk.fit(X,y_not_binary) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81016631  1.39369878 -1.68738578 -1.51899135]\n",
      " [ 0.13037985 -1.2463382   0.78919477 -0.88943988]\n",
      " [ 0.01299039 -0.1445346   1.86317337  2.69887272]]\n",
      "Accuracy of:  0.9266666666666666\n",
      "CPU times: user 3.39 ms, sys: 1.32 ms, total: 4.71 ms\n",
      "Wall time: 3.59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actually, we aren't quite as good as the lib linear implementation\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='liblinear',n_jobs=1) \n",
    "\n",
    "lr_sk.fit(X,y_not_binary) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liblinear is a great toolkit for linear modeling (from national Taiwan University) and the paper can be found here:\n",
    "- https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf\n",
    "\n",
    "Actually, this solves a slightly different problem (than maximum likelihood) to make it extremely fast. **If you are a student taking this class for 7000 credit, you will need to slightly alter your method to get the same speed and result as sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# its still faster! Can we fix that?\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class ParallelMultiClassLogisticRegression(MultiClassLogisticRegression):\n",
    "    @staticmethod\n",
    "    def par_logistic(yval,eta,iters,C,X,y):\n",
    "        y_binary = y==yval # create a binary problem\n",
    "        # train the binary classifier for this class\n",
    "        hblr = BFGSBinaryLogisticRegression(eta,iters,C)\n",
    "        hblr.fit(X,y_binary)\n",
    "        return hblr\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        backend = 'threading' # can also try 'multiprocessing'\n",
    "        \n",
    "        self.classifiers_ = Parallel(n_jobs=-1,backend=backend)(\n",
    "            delayed(self.par_logistic)(yval,self.eta,self.iters,self.C,X,y) for yval in self.unique_)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "plr = ParallelMultiClassLogisticRegression(eta=0.1,iterations=10,C=0.0001)\n",
    "plr.fit(X,y_not_binary)\n",
    "print(plr)\n",
    "\n",
    "yhat = plr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Please note that the overhead of parallelization is not worth it for this problem!!\n",
    "\n",
    "**When would it make sense???**\n",
    "___\n",
    "# Next Time: In Class Assignment, SVMs\n",
    "___\n",
    "# Next Next Time: Neural Networks\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
