{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Advanced  Optimization for Machine Learning\n",
    "From previous notebooks, we know that the logistic regression update equation is given by:\n",
    "$$ \\underbrace{w_j}_{\\text{new value}} \\leftarrow \\underbrace{w_j}_{\\text{old value}} + \\eta \\underbrace{\\sum_{i=1}^M (y^{(i)}-\\hat{y}^{(i)})x^{(i)}_j}_{\\text{gradient}} $$\n",
    "\n",
    "$$ w \\leftarrow w + \\eta \\sum_{i=1}^M (y^{(i)}-\\hat{y}^{(i)})x^{(i)} $$\n",
    "\n",
    "Which can be made more generic by denoting the objective function as $l(w)$:\n",
    "$$ w \\leftarrow w + \\eta \\nabla l(w)$$\n",
    "\n",
    "One problem is that we still need to set the value of $\\eta$, which can drastically change the performance of the optimization algorithm. If $\\eta$ is too large, the algorithm might be unstable. If $\\eta$ is too small, it might take forever to converge.\n",
    "$$ w \\leftarrow w + \\underbrace{\\eta}_{\\text{best step?}} \\nabla l(w) $$\n",
    "\n",
    "We can solve this issue by performing a line search for the best value of $\\eta$ along the direction of the gradient, as denoted by:\n",
    "\n",
    "$$ \\eta \\leftarrow \\arg\\min_\\eta \\sum_{i=1}^M (y^{(i)}-\\hat{y}^{(i)})^2 $$\n",
    "\n",
    "# Optimizing Logistic Regression via Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(np.int) # make problem binary\n",
    "Xb = np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "\n",
    "def accuracy(y,yhat):\n",
    "    return sum(yhat==y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.7 ms, sys: 2.68 ms, total: 39.4 ms\n",
      "Wall time: 39.6 ms\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ -58.04278718]\n",
      " [-101.88713425]\n",
      " [ -89.9532887 ]\n",
      " [ 149.56437443]\n",
      " [ 128.26409526]]\n",
      "Accuracy of:  0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eclarson/anaconda/envs/MLEnv/lib/python3.5/site-packages/ipykernel/__main__.py:15: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "# from last time, our logistic regression algorithm is given by:\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def sigmoid(self,theta):\n",
    "        return 1/(1+np.exp(-theta))\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        return self.sigmoid(X @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5).astype(np.int).ravel() #return the actual prediction\n",
    "    \n",
    "    # vectorized the fit function\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape    \n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1))\n",
    "        for _ in range(self.iters):\n",
    "            ydiff = y-self.predict_proba(X).ravel() # get y difference\n",
    "            gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            self.w_ += gradient.reshape(self.w_.shape)*self.eta # add with learning rate\n",
    "        \n",
    "\n",
    "blr = BinaryLogisticRegression(0.1,500)\n",
    "\n",
    "%time blr.fit(Xb,y)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(Xb)\n",
    "print('Accuracy of: ',accuracy(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.1 ms, sys: 2.25 ms, total: 43.3 ms\n",
      "Wall time: 43.8 ms\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-103.5757535 ]\n",
      " [-227.88065221]\n",
      " [-249.2494365 ]\n",
      " [ 380.44490565]\n",
      " [ 274.40859102]]\n",
      "Accuracy of:  0.966666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eclarson/anaconda/envs/MLEnv/lib/python3.5/site-packages/ipykernel/__main__.py:12: RuntimeWarning: overflow encountered in exp\n",
      "/Users/eclarson/anaconda/envs/MLEnv/lib/python3.5/site-packages/ipykernel/__main__.py:15: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # define custom line search for problem\n",
    "        def line_search_function(eta,X,y,w,grad):\n",
    "            wnew = w+grad*eta\n",
    "            yhat = (1/(1+np.exp(-X @ wnew)))>0.5\n",
    "            return np.sum((y-yhat)**2)\n",
    "        \n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1))\n",
    "        for _ in range(self.iters):\n",
    "            # get gradient direction\n",
    "            ydiff = y-self.predict_proba(X).ravel() # get y difference\n",
    "            gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/10,self.eta*10), #bounds to optimize\n",
    "                                  args=(X,y,self.w_,gradient), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "            \n",
    "\n",
    "lslr = LineSearchLogisticRegression(0.1,60)\n",
    "\n",
    "%time lslr.fit(Xb,y)\n",
    "\n",
    "yhat = lslr.predict(Xb)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy(y,yhat))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improvement in the accuracy, but did take a little while longer (this is because $\\eta$ was chosen well in the initial example). \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "Sometimes the gradient calcualtion is too computational:\n",
    "$$ w \\leftarrow w + \\eta \\sum_{i=1}^M (y^{(i)}-\\hat{y}^{(i)})x^{(i)} $$\n",
    "\n",
    "Instead, we can approximate the gradient using one instance, this is called stochastic gradient descent (SGD) because the steps can appear somewhat random.\n",
    "$$ w \\leftarrow w + \\eta \\underbrace{(y^{(i)}-\\hat{y}^{(i)})x^{(i)}}_{\\text{approx. gradient}} \\text{,   where   } i\\in M$$\n",
    "\n",
    "Let's code up the SGD example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 ms, sys: 3.53 ms, total: 25.9 ms\n",
      "Wall time: 26.6 ms\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-1.55739978]\n",
      " [-2.86353535]\n",
      " [-3.15928981]\n",
      " [ 4.41201547]\n",
      " [ 4.16543722]]\n",
      "Accuracy of:  0.98\n"
     ]
    }
   ],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape    \n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1))\n",
    "        for _ in range(self.iters):\n",
    "            idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "            ydiff = y[idx]-self.predict_proba(X[idx]) # get y difference\n",
    "            gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "            self.w_ += gradient.reshape(self.w_.shape)*self.eta # add with learning rate\n",
    "        \n",
    "slr = StochasticLogisticRegression(0.1,1000)\n",
    "\n",
    "%time slr.fit(Xb,y)\n",
    "\n",
    "yhat = slr.predict(Xb)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy(y,yhat))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Optimizing with Second Order Derivatives\n",
    "First, let's look at the one dimensioanl case when we have a function $l(w)$ where w is a scalar. The optimal value of w is given by:\n",
    "\n",
    "$$ w \\leftarrow w - \\underbrace{[\\frac{\\partial^2}{\\partial w}l(w)]^{-1}}_{\\text{inverse 2nd deriv}}\\underbrace{\\frac{\\partial}{\\partial w}l(w)}_{\\text{derivative}}  $$\n",
    "\n",
    "Note that if $l(w)$ is a quadrativ function, this solution converges in a single step!\n",
    "\n",
    "\n",
    "### Aside: an example with the second derivative:\n",
    "Say $l(w)=2w^2+4w+5$, and we want to minimize the function. We have that:\n",
    "\n",
    "$\\frac{\\partial}{\\partial w}l(w)=4w+4$\n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial^2}{\\partial w}l(w)=4$. \n",
    "\n",
    "Therefore, if we choose $w_{start}=0$, we have: \n",
    "\n",
    "$\\frac{\\partial}{\\partial w}l(0)=4$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial^2}{\\partial w}l(0)=4$ \n",
    "\n",
    "So the update becomes\n",
    "\n",
    "$w \\leftarrow w_{start} - \\frac{1}{4}4 = -1$\n",
    "\n",
    "The solution is found in one step. This works for any initial value of $w_{start}$. Let's verify that the solution worked graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10b828198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b828160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "w = np.linspace(-7,5,100)\n",
    "l = 2*w**2+4*w+5\n",
    "plt.plot(w,l)\n",
    "plt.text(-1,2.5,'$\\leftarrow$found minimum',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "But how do we translate this over to objective funtions with more than one variable? We need a second derivative... enter, the hessian:\n",
    "\n",
    "$$ \\nabla^2 l(w) = \\mathbf{H}[l(w)]   $$\n",
    "\n",
    "$$  \\mathbf{H}[l(w)] =  \\begin{bmatrix}\n",
    "        \\frac{\\partial^2}{\\partial w_1}l(w) &  \\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_2}l(w) & \\ldots     & \\frac{\\partial}{\\partial w_1}\\frac{\\partial}{\\partial w_N}l(w)  \\\\\n",
    "        \\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_1}l(w)  & \\frac{\\partial^2}{\\partial w_2}l(w) &  \\ldots     & \\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_N}l(w)  \\\\\n",
    "        &  \\vdots &\\\\\n",
    "        \\frac{\\partial}{\\partial w_N}\\frac{\\partial}{\\partial w_1}l(w)  & \\frac{\\partial}{\\partial w_N}\\frac{\\partial}{\\partial w_2}l(w) &  \\ldots     & \\frac{\\partial^2}{\\partial w_N}l(w) \\\\\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ w \\leftarrow w + \\eta \\cdot \\underbrace{\\mathbf{H}[l(w)]^{-1}}_{\\text{inverse Hessian}}\\cdot\\underbrace{\\nabla l(w)}_{\\text{gradient}}$$\n",
    "\n",
    "For logistic regression\n",
    "$$ \\mathbf{H}_{j,k}[l(w)] = -\\sum_{i=1}^M g(x^{(i)})(1-g(x^{(i)}){x_k}^{(i)}{x_j}^{(i)}   $$\n",
    "\n",
    "You can see the full derivation of the Hessian in my notes here:\n",
    "- https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/HessianCalculation.pdf\n",
    "\n",
    "$$ \\mathbf{H}[l(w)] =  X^T \\cdot \\text{diag}[g(x^{(i)})(1-g(x^{(i)})]\\cdot X $$\n",
    "\n",
    "$$ w \\leftarrow w + \\eta [X^T \\cdot \\text{diag}[g(x^{(i)})(1-g(x^{(i)})]\\cdot X]^{-1}\\cdot X*y_{diff}$$\n",
    "\n",
    "So let's code this up using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.23 ms, sys: 1.72 ms, total: 8.95 ms\n",
      "Wall time: 5.35 ms\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-14.08319984]\n",
      " [ -1.59714009]\n",
      " [ -2.20181703]\n",
      " [  3.81731771]\n",
      " [  7.04317257]]\n",
      "Accuracy of:  0.98\n"
     ]
    }
   ],
   "source": [
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1))\n",
    "        for _ in range(self.iters):\n",
    "            g = self.predict_proba(X).ravel() # get sigmoid value for all classes\n",
    "            hessian = X.T @ np.diag(g*(1-g)) @ X # calculate the hessian\n",
    "            \n",
    "            ydiff = y-g # get y difference\n",
    "            gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)#/len(y)\n",
    "            \n",
    "            self.w_ += self.eta * np.linalg.pinv(hessian) @ gradient # add with learning rate\n",
    "            \n",
    "hlr = HessianBinaryLogisticRegression(1,5) # note that we need only a few iterations here\n",
    "\n",
    "%time hlr.fit(Xb,y)\n",
    "yhat = hlr.predict(Xb)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "But, as we discussed, the hessian can sometimes be ill formed for these problems and can also be highly computational. Thus, we need to approximate the Hessain, and also use some heuristics (like momentum) to better control the steps we make and directions we use. \n",
    "\n",
    "We won't explicitly program the BFGS algorithm--instead we can take advantage of scipy's calculations to do it for us. \n",
    "\n",
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 ms, sys: 971 µs, total: 8.99 ms\n",
      "Wall time: 6.44 ms\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.11114818]\n",
      " [-0.2677297 ]\n",
      " [-0.37792715]\n",
      " [ 0.49339159]\n",
      " [ 0.28635464]]\n",
      "Accuracy of:  0.98\n"
     ]
    }
   ],
   "source": [
    "# for this, we won't perform our own BFGS implementation (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        def objective_function(w,X,y):\n",
    "            g = (1/(1+np.exp(-X @ w)))\n",
    "            return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "        \n",
    "        def objective_gradient(w,X,y):\n",
    "            g = (1/(1+np.exp(-X @ w)))\n",
    "            yhat = g #(g>0.5).astype(np.int)\n",
    "            ydiff = y-yhat # get y difference\n",
    "            gradient = np.sum(X * ydiff[:,np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)/len(y)\n",
    "            return -gradient\n",
    "        \n",
    "        self.w_ = fmin_bfgs(objective_function, \n",
    "                            np.zeros((num_features,1)), \n",
    "                            fprime=objective_gradient, \n",
    "                            args=(X,y),gtol=1e-03,\n",
    "                            maxiter=self.iters,\n",
    "                            disp=False).reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSBinaryLogisticRegression(0.1,2) # note that we need only a few iterations here\n",
    "\n",
    "%time bfgslr.fit(Xb,y)\n",
    "yhat = bfgslr.predict(Xb)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS for Multiclass Logistic Regression\n",
    "Now let's add BFGS to non-binary classification. As before, we will use one-versus-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 426 µs, total: 12.3 ms\n",
      "Wall time: 6.83 ms\n",
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.52228397  0.94092592  2.81252107 -4.20625865 -1.90723814]\n",
      " [ 0.78427265  1.16013913 -2.66583412  0.56585043 -2.1566517 ]\n",
      " [-2.31059509 -5.09367038 -5.38233776  8.07959037  5.94573873]]\n",
      "Accuracy of:  0.98\n"
     ]
    }
   ],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = BFGSBinaryLogisticRegression(self.eta,self.iters)\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y_not_binary = ds.target # note problem is NOT binary anymore, there are three classes!\n",
    "Xb = np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "\n",
    "lr = MultiClassLogisticRegression(1,10)\n",
    "%time lr.fit(Xb,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(Xb)\n",
    "print('Accuracy of: ',accuracy(y_not_binary,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 1.01 ms, total: 29.2 ms\n",
      "Wall time: 17 ms\n",
      "[[-0.44524582  0.89469401 -2.32542777 -0.97869151]\n",
      " [-0.18587061 -2.11489439  0.69770617 -1.25139648]\n",
      " [-0.39444575 -0.5132796   2.93082545  2.41710589]]\n",
      "Accuracy of:  0.953333333333\n"
     ]
    }
   ],
   "source": [
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='lbfgs',n_jobs=1) # all params default\n",
    "# note that sklearn is optimized for using the liblinear library with logistic regression\n",
    "# ...and its faster than our implementation here\n",
    "\n",
    "%time lr_sk.fit(X,y_not_binary) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.27 ms, sys: 2.4 ms, total: 5.67 ms\n",
      "Wall time: 4.48 ms\n",
      "[[ 0.41498833  1.46129739 -2.26214118 -1.0290951 ]\n",
      " [ 0.41663969 -1.60083319  0.57765763 -1.38553843]\n",
      " [-1.70752515 -1.53426834  2.47097168  2.55538211]]\n",
      "Accuracy of:  0.96\n"
     ]
    }
   ],
   "source": [
    "# actually, we aren't nearly as good as the lib linear implementation\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='liblinear',n_jobs=1) \n",
    "\n",
    "%time lr_sk.fit(X,y_not_binary) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 418 µs, total: 16 ms\n",
      "Wall time: 8.59 ms\n",
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.52228397  0.94092592  2.81252107 -4.20625865 -1.90723814]\n",
      " [ 0.78427265  1.16013913 -2.66583412  0.56585043 -2.1566517 ]\n",
      " [-2.31059509 -5.09367038 -5.38233776  8.07959037  5.94573873]]\n",
      "Accuracy of:  0.98\n"
     ]
    }
   ],
   "source": [
    "# its still faster! Can we fix that?\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def par_logistic(yval,eta,iters,X,y):\n",
    "    y_binary = y==yval # create a binary problem\n",
    "    # train the binary classifier for this class\n",
    "    hblr = BFGSBinaryLogisticRegression(eta,iters)\n",
    "    hblr.fit(X,y_binary)\n",
    "    return hblr\n",
    "\n",
    "class ParallelMultiClassLogisticRegression(MultiClassLogisticRegression):\n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        backend = 'threading' #'multiprocessing'\n",
    "        self.classifiers_ = Parallel(n_jobs=1,backend=backend)(\n",
    "            delayed(par_logistic)(yval,self.eta,self.iters,X,y) for yval in self.unique_)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "plr = ParallelMultiClassLogisticRegression(1,10)\n",
    "%time plr.fit(Xb,y_not_binary)\n",
    "print(plr)\n",
    "\n",
    "yhat = plr.predict(Xb)\n",
    "print('Accuracy of: ',accuracy(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Please note that the overhead of parallelization is not worth it for this problem!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [MLEnv]",
   "language": "python",
   "name": "Python [MLEnv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
