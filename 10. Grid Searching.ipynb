{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Searching\n",
    "In this notebook we will look at some very basic techniques for grid searching parameters in scikit-learn.\n",
    "\n",
    "To start things off, we will load a medium sized dataset (MNIST) as we have done many times in the past. \n",
    "\n",
    "To cover:\n",
    "- Setting up Grid Search in scikit\n",
    "- Using with pipelines\n",
    "- Memory replication\n",
    "- The problems of Parallelsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "# more data for handwriting recognition?\n",
    "# Let's use Raschka's implementation for using the mnist dataset:\n",
    "# https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('data/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('data/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'ovr',\n",
       " 'n_jobs': 1,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# we want to know some parameters of the logistic regression\n",
    "# in order to use it with the current data\n",
    "\n",
    "# here are some parameters for tuning\n",
    "LogisticRegression().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Clearly, we should adjust the regularization parameter, C. We should probably also look at the type of regularization, L1-norm versus L2-norm for this application.\n",
    "\n",
    "In order to adjust these parameters, we also need a scoring function. For this application, it makes sense to choose the model with the best accuracy, although other cost metrics/evaluation criteria could be applied. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9203\n",
      "CPU times: user 1min 21s, sys: 258 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# how long should it take to fit one model?\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "print(accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it takes about one minute per model to create. Running a grid search, then will take some time. To speed things up, let's use a smaller number of cross validation folds (like 2 or 3). Also let's limit the number of parameters investigated to about four sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
      "       error_score='raise',\n",
      "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'penalty': ['l1', 'l2'], 'C': [10, 1]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "grid = {'C':[10,1], # adjust regularization cost\n",
    "        'penalty':['l1','l2'], #adjust weights regularization\n",
    "       }\n",
    "# this will create a parameter grid of 2x2=4 combinations\n",
    "\n",
    "cv_strat = StratifiedKFold(n_splits=3) # and three splits per combination\n",
    "# so this will make 4x3=12 different models to create\n",
    "\n",
    "search = GridSearchCV(estimator=LogisticRegression(),\n",
    "                      param_grid=grid,\n",
    "                      cv=cv_strat,\n",
    "                      n_jobs=-1, # run in parallel \n",
    "                     )\n",
    "\n",
    "# this is the object that will perform the search\n",
    "print(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 1.44 s, total: 1min 6s\n",
      "Wall time: 8min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [10, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# now lets fit it to the training data and get a good estimator! Maybe!\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this process was running, this is how my machine resources were distributed for the different python workers:\n",
    "![resources](PDF_Slides/resources.png)\n",
    "\n",
    "As you can see, there were four different workers running on the separate cores. Each core was utilizing about 90-97% of the core and each core consumed about 500MB of RAM. That is because the MNIST data was copied to each worker (memory replication). That's not ideal, but makes sense if this were a distributed environment where each machine needed a copy of the memory. It also makes the grid search much easier to program and use because we don't need to worry about locking memory between the different processes (which could potentially be a big slow down to search performance). Since my machine has 8GB of RAM, this fits nicely into memory even when replicated. \n",
    "\n",
    "The training happened in parallel, so it only took about 8 minutes to finish! That means the training for the 12 different logistic regression models (about ~1.5 minute per model) did not take a full 18 minutes. It might have taken (12 models x 1.5 mins) / 4 cores=4.5 mins, but each training process takes more or less time to converge depending on the solver. That means the overhead (in terms of CPU) was not too terrible and the processing sped up nearly 2 times! That is parallelism! (in fact, it was embarassingly parallel, so *meh*, but its okay to get excited when computation gets faster).\n",
    "\n",
    "We will return to memory and CPU tradeoffs later, but for now lets see how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eclarson/anaconda/envs/MLEnv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:662: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.91168, std: 0.00184, params: {'penalty': 'l1', 'C': 10},\n",
       " mean: 0.91208, std: 0.00202, params: {'penalty': 'l2', 'C': 10},\n",
       " mean: 0.91472, std: 0.00214, params: {'penalty': 'l1', 'C': 1},\n",
       " mean: 0.91372, std: 0.00260, params: {'penalty': 'l2', 'C': 1}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will be a deprecated property, but it gives a nice summary\n",
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(166.47111829121908, {'C': 10, 'penalty': 'l1'}),\n",
       " (212.12199465433756, {'C': 10, 'penalty': 'l2'}),\n",
       " (80.596707344055176, {'C': 1, 'penalty': 'l1'}),\n",
       " (85.935367345809937, {'C': 1, 'penalty': 'l2'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also look at the mean time it took for the parameters\n",
    "# this accesses the cv_results_ property that has a lot more info in it\n",
    "list(zip(search.cv_results_['mean_fit_time'],search.cv_results_['params']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the C=10 setting took much longer on average than smaller C values. That accounts for the increased convergence time we saw above.\n",
    "\n",
    "So the different parameters did quite well in terms of accuracy on the validation folds (~91-92%). How well do they perform on the test set that we explicitly left out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9195\n"
     ]
    }
   ],
   "source": [
    "# its each to simply use the best grid search estimator.\n",
    "# We simply treat the search like an estimator\n",
    "yhat = search.predict(X_test)\n",
    "print(accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all-in-all the estimator performed about the same regardless of the input parameters. 92% is nothing to be ashamed of on MNIST, but it certainly is nothing to be proud of. Its much less than state of the art and its much less than we performed with a multi-layer perceptron. We won't try to increase this value too much though, because the point of this notebook is to exemplify grid search strategy.\n",
    "\n",
    "Now let's try grid searching with a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
      "       error_score='raise',\n",
      "       estimator=Pipeline(steps=[('pca_pre', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid={'clf__C': [1, 0.1], 'pca_pre__n_components': [100, 10]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "        ('pca_pre',PCA()),\n",
    "        ('clf',LogisticRegression())\n",
    "    ])\n",
    "\n",
    "# now we can access individual models in the pipe\n",
    "# using the __ naming\n",
    "grid = {'clf__C':[1,0.1], # adjust regularization cost\n",
    "        'pca_pre__n_components':[100,10], #num components\n",
    "       }\n",
    "# this will create a parameter grid of 2x2=4 combinations\n",
    "\n",
    "search = GridSearchCV(estimator=model,\n",
    "                      param_grid=grid,\n",
    "                      cv=cv_strat,\n",
    "                      n_jobs=-1, # run in parallel \n",
    "                     )\n",
    "\n",
    "# this is the object that will perform the search\n",
    "print(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 1.27 s, total: 51.7 s\n",
      "Wall time: 2min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('pca_pre', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'clf__C': [1, 0.1], 'pca_pre__n_components': [100, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# now lets refit and see if the PCA helped\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eclarson/anaconda/envs/MLEnv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:662: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.90727, std: 0.00270, params: {'clf__C': 1, 'pca_pre__n_components': 100},\n",
       " mean: 0.77648, std: 0.00404, params: {'clf__C': 1, 'pca_pre__n_components': 10},\n",
       " mean: 0.90380, std: 0.00235, params: {'clf__C': 0.1, 'pca_pre__n_components': 100},\n",
       " mean: 0.77440, std: 0.00443, params: {'clf__C': 0.1, 'pca_pre__n_components': 10}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we were able to solve the problem quickly. But really, we need to do a broader search of the parameters if we want things to improve. Lets expand the limits we were working on. This time we will need to create:\n",
    "- 3 pca params x 3 costs x 2 penalties x 3 folds = 54 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 5s, sys: 8.8 s, total: 5min 14s\n",
      "Wall time: 31min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we will use slightly different dictionary construction here\n",
    "# this might be easier to write\n",
    "grid = dict(pca_pre__n_components=[100, 200, 500],\n",
    "            clf__C=[1e-4, 1e-2, 1.0],\n",
    "            clf__penalty=['l1', 'l2'])\n",
    "\n",
    "# this is certainly a lot more parameters!!\n",
    "search = GridSearchCV(estimator=model,\n",
    "                      param_grid=grid,\n",
    "                      cv=cv_strat,\n",
    "                      n_jobs=-1, # run in parallel \n",
    "                     )\n",
    "\n",
    "# now lets refit and see if the increased search space helped\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eclarson/anaconda/envs/MLEnv/lib/python3.5/site-packages/sklearn/model_selection/_search.py:662: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.20925, std: 0.00025, params: {'clf__penalty': 'l1', 'clf__C': 0.0001, 'pca_pre__n_components': 100},\n",
       " mean: 0.20925, std: 0.00025, params: {'clf__penalty': 'l1', 'clf__C': 0.0001, 'pca_pre__n_components': 200},\n",
       " mean: 0.20925, std: 0.00025, params: {'clf__penalty': 'l1', 'clf__C': 0.0001, 'pca_pre__n_components': 500},\n",
       " mean: 0.81565, std: 0.00683, params: {'clf__penalty': 'l2', 'clf__C': 0.0001, 'pca_pre__n_components': 100},\n",
       " mean: 0.81670, std: 0.00715, params: {'clf__penalty': 'l2', 'clf__C': 0.0001, 'pca_pre__n_components': 200},\n",
       " mean: 0.81670, std: 0.00701, params: {'clf__penalty': 'l2', 'clf__C': 0.0001, 'pca_pre__n_components': 500},\n",
       " mean: 0.87803, std: 0.00385, params: {'clf__penalty': 'l1', 'clf__C': 0.01, 'pca_pre__n_components': 100},\n",
       " mean: 0.87795, std: 0.00380, params: {'clf__penalty': 'l1', 'clf__C': 0.01, 'pca_pre__n_components': 200},\n",
       " mean: 0.87795, std: 0.00380, params: {'clf__penalty': 'l1', 'clf__C': 0.01, 'pca_pre__n_components': 500},\n",
       " mean: 0.89157, std: 0.00321, params: {'clf__penalty': 'l2', 'clf__C': 0.01, 'pca_pre__n_components': 100},\n",
       " mean: 0.89468, std: 0.00375, params: {'clf__penalty': 'l2', 'clf__C': 0.01, 'pca_pre__n_components': 200},\n",
       " mean: 0.89577, std: 0.00391, params: {'clf__penalty': 'l2', 'clf__C': 0.01, 'pca_pre__n_components': 500},\n",
       " mean: 0.90772, std: 0.00274, params: {'clf__penalty': 'l1', 'clf__C': 1.0, 'pca_pre__n_components': 100},\n",
       " mean: 0.91298, std: 0.00185, params: {'clf__penalty': 'l1', 'clf__C': 1.0, 'pca_pre__n_components': 200},\n",
       " mean: 0.91353, std: 0.00206, params: {'clf__penalty': 'l1', 'clf__C': 1.0, 'pca_pre__n_components': 500},\n",
       " mean: 0.90745, std: 0.00275, params: {'clf__penalty': 'l2', 'clf__C': 1.0, 'pca_pre__n_components': 100},\n",
       " mean: 0.91227, std: 0.00215, params: {'clf__penalty': 'l2', 'clf__C': 1.0, 'pca_pre__n_components': 200},\n",
       " mean: 0.91285, std: 0.00226, params: {'clf__penalty': 'l2', 'clf__C': 1.0, 'pca_pre__n_components': 500}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result are not so great... But this happens many times in tuning. The above task took 30 minutes to complete. \n",
    "___\n",
    "\n",
    "Now let's get a quick taste of what we having coming up next. We will use dask-learn (its experimental and may not last, but let's use it anyway) to perform the exact same grid search. The only difference here is the lazily evaluated nature of dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 21min 14.8s\n",
      "[########################################] | 100% Completed |  4min 56.6s\n",
      "CPU times: user 1h 11min 4s, sys: 1min 44s, total: 1h 12min 48s\n",
      "Wall time: 26min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from dask.diagnostics import ProgressBar # nice convenience function\n",
    "from dklearn.grid_search import GridSearchCV as DaskGridSearchCV\n",
    "\n",
    "# The use of Dask introduces lazy computations\n",
    "dsearch = DaskGridSearchCV(estimator=model,\n",
    "                           param_grid=grid,\n",
    "                           cv=3, \n",
    "                          )\n",
    "\n",
    "# now lets refit and see if we can increase the search time\n",
    "with ProgressBar():\n",
    "    dsearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example took 25 minutes to run, versus the 30 minutes for sklearn. Why? Because the lazy evaluation of dask allowed us to cache the result of the PCA, rather than recalcualting it each time like the naive implementation by scikit-learn (overall the PCA was only a modest CPU user). Scikit's implementation isn't neccessarily bad, but recalcuating PCA is a needless step in this example (this does increase memory footprint however). \n",
    "For more explanation, take a look at this example from **Jim Crist**:\n",
    "- http://matthewrocklin.com/blog/work/2016/07/12/dask-learn-part-1\n",
    "\n",
    "And his graphic:\n",
    "![Dask profile](https://mrocklin.github.com/blog/images/grid_search_schedule.gif)\n",
    "\n",
    "Using dask, the cpu is still running many different processes, but not replicating python processes to do it. In Dask, we get:\n",
    "![Dask resources](PDF_Slides/resource_dask.png)\n",
    "\n",
    "Numpy matrices (the MNIST data) has still been replicated and we are still running with many threads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [MLEnv]",
   "language": "python",
   "name": "Python [MLEnv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
