{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Analysis in Python\n",
    "In this notebook we will cover:\n",
    "- Reading document data into memory\n",
    "- Creating bag of words features\n",
    "- Creating smoothed tf-idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from contextlib import closing\n",
    "\n",
    "# get API key saved on hardrive\n",
    "with open('../NYTimesAPI.txt') as f:\n",
    "    api_key = f.read() # read in my private key (sorry, not in this repo ¯\\_(ツ)_/¯ )\n",
    "    \n",
    "# make base URL and dictionary of get request key/values\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "payload = {'api-key': api_key, 'q':'super bowl'} # key/values for get request (look up in api, there are lots)\n",
    "\n",
    "# Perform the actual request\n",
    "with closing(requests.get(url,params=payload)) as r:\n",
    "    articles = r.json()\n",
    "    print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OR we can load an example query like this\n",
    "# run this block of code if you can't run anything else\n",
    "import json \n",
    "with open('data/nytime.json') as f:\n",
    "    articles = json.loads(f.read())\n",
    "    \n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the summary text from each article\n",
    "#  lead_paragraph  is no snippet\n",
    "summary_text = [x['snippet'] for x in articles['response']['docs']]\n",
    "summary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting document data into different representations\n",
    "First lets go through and count the unique words in each opening sentence (that is what the NYTimes give us for free).\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer() # an object capable of counting words in a document!\n",
    "\n",
    "# count_vect.fit(summary_text)\n",
    "# count_vect.transform(summary_text)\n",
    "bag_words = count_vect.fit_transform(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bag_words.shape) # this is a sparse matrix\n",
    "print('=========')\n",
    "print(bag_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Test: ML01b.3: \n",
    "Do you expect the vocabulary from the articles above to be:\n",
    "- A. Greater than 1M words\n",
    "- B. Greater than 10,000 words\n",
    "- C. Fewer than 10,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(count_vect.vocabulary_))\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can still look at the data using an inverse transform\n",
    "# but we lose the ordering of the words (after all its just a bag of wrods model)\n",
    "count_vect.inverse_transform(bag_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's create a pandas API out of this\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "df = pd.DataFrame(data=bag_words.toarray(),columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df # display the full bag of words matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out 10 most common words in our data\n",
    "df.sum().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out 10 least common words in our data\n",
    "df.sum().sort_values()[:10] # small sample size means most words occur one time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Conversion\n",
    "We have a very small sample of data, but lets covert to tf-idf for the sake of programming it. Recall that Tf-idf transformation (default in `sklearn` is):\n",
    "\n",
    "$$ \\text{tf}(t,d) = f_{td}\\text{, } t\\in T \\text{ and } d \\in D $$\n",
    "\n",
    "$$ \\text{idf}(t,d) = \\log{\\frac{|D|}{|n_t|}}\\text{, where } n_t=d\\in D \\text{ with } t\\in d $$\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf}(t,d) \\cdot (1+\\text{idf}(t,d))$$\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer() # an object capable of counting words in a document!\n",
    "\n",
    "tfidf_mat = tfidf_vect.fit_transform(summary_text) # that's it! its converted!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to pandas to get better idea about the data\n",
    "df = pd.DataFrame(data=tfidf_mat.toarray(),columns=tfidf_vect.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out 10 words with max tfidf, normalized by document occurrence\n",
    "df.max().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with (a bit) more data\n",
    "What if we do not have the memory to deal with dense matrix representatioan and we need to keep it sparse?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "bunch = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `bunch` object returned from sklearn is similar to a python dictionary. We can access different fields of the object with keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bunch.data[0]) # we should split this up by newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = round(np.random.rand()*len(bunch.data))\n",
    "print(\"\\n\".join(bunch.data[idx].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "news_tfidf = tfidf_vect.fit_transform(bunch.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create pandas dataframe\n",
    "vec = news_tfidf.max(axis=0)\n",
    "df  = pd.DataFrame(data=vec.toarray(),columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# largest tfidf \n",
    "df.max().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now lets do the transformation with a smaller vocabulary\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english',\n",
    "                             max_df=0.01,\n",
    "                             min_df=4)\n",
    "news_tfidf = tfidf_vect.fit_transform(bunch.data) \n",
    "print(news_tfidf.shape)\n",
    "vec=news_tfidf.max(axis=0)\n",
    "df = pd.DataFrame(data=vec.toarray(),columns=tfidf_vect.get_feature_names())\n",
    "df.max().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using your own vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in scrabble dictionary from file\n",
    "with open('data/ospd.txt') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "    \n",
    "# now lets do the transformation with a custom vocabulary\n",
    "tfidf_vect = TfidfVectorizer(vocabulary=vocab)\n",
    "news_tfidf = tfidf_vect.fit_transform(bunch.data) \n",
    "print(news_tfidf.shape)\n",
    "vec=news_tfidf.max(axis=0)\n",
    "df = pd.DataFrame(data=vec.toarray(),columns=tfidf_vect.get_feature_names())\n",
    "df.max().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looking for how to do a word cloud? Check this out:\n",
    "- https://github.com/amueller/word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MLEnv]",
   "language": "python",
   "name": "conda-env-MLEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
